{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "file_id = '1DubBO3fagYccVuKwrIVa4zGvsCzW7FCr'\n",
    "destination = './data/ckpt.pth'\n",
    "if os.path.exists(destination) == False:    #destination が存在していない場合のみDL\n",
    "  download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'config.Config'>\n",
      "cpu\n",
      "Traceback (most recent call last):\n",
      "  File \"run_eval.py\", line 16, in <module>\n",
      "    state_dict = torch.load(f'{Config.data_dir}/{Config.fn}.pth')\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 386, in load\n",
      "    return _load(f, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 573, in _load\n",
      "    result = unpickler.load()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 536, in persistent_load\n",
      "    deserialized_objects[root_key] = restore_location(obj, location)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 119, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 95, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/torch/serialization.py\", line 79, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    }
   ],
   "source": [
    "!python3 run_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
